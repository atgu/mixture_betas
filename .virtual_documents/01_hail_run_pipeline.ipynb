import hail

import hailtop.batch as hb



import pandas as pd







alt_splice_sites=pd.read_csv(data_directory+'alt_splice_sites_filtered_for_popPSI.csv.gz', compression='gzip')





gtex_tissue_list=sorted(list(set(alt_splice_sites.tissue)))





b = hb.Batch(backend=backend, name='emp_bayes')
num_jobs = 25 #samples every 100 index



#scale to all lists of simulations
simulation_type=['single_1',
                'single_2',
                'double_1','double_2',
                 'triple_1', 'triple_2']
data_amt=['low', 'mid', 'high']
i=0
k=0
for i in range(len(simulation_type)):

    for k in range(len(data_amt)):
        l = b.new_job(name='all_data_ammounts')
        l.memory('20Gi')
        l.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
        tissue='test_tissue'
    #localizing the files
        input_script = "/home/run_optimizer_alt_ss_w_mixed_effects.py"    
        
        input_file_tissue = b.read_input("gs://sqtl_gtex/files/testing/mixture_testing/inputs_simulations/"+data_amt[k]+"_NVE_"+simulation_type[i]+"_counts.csv.gz")
        
        input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/testing/mixture_testing/inputs_simulations/filtering_exons_in_test_sample.csv.gz")
            
        
        for job_index in range(num_jobs):
            
            j = b.new_job(name='run_emp_bayes_test_')
            j.memory('20Gi')
            j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
        
            j.command(f'python3 {input_script} \
                            --tissue {tissue} \
                                --filter_file {input_file_to_filter_events} \
                                --tissue_file {input_file_tissue} \
                                --output_file {j.output} \
                                --num_jobs {num_jobs} \
                                --job_index {job_index}')
        
            b.write_output(j.output, f'gs://sqtl_gtex/files/testing/mixture_testing/outputs_simulations_new_pipeline/{data_amt[k]}_{simulation_type[i]}_EF_FPR_5_alphas_betas-{job_index}.csv')
        
            
b.run()



    


b = hb.Batch(backend=backend, name='emp_bayes')
directory='gs://sqtl_gtex/files/testing/mixture_testing/outputs_simulations_new_pipeline/'

j = b.new_job(name='run_emp_bayes_test_')
j.memory('20Gi')
j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
for i in range(len(simulation_type)):
    for k in range(len(data_amt)):
        files_given_sim=[]
        
        for job_index in range(num_jobs):
    
            file=data_amt[k]+'_'+simulation_type[i]+'_EF_FPR_5_alphas_betas-'+str(job_index)+'.csv'
            
            o_file=b.read_input(directory+file)

            files_given_sim.append(o_file)
            new_file_name=data_amt[k]+'_'+simulation_type[i]+'_EF_FPR_5_alphas_betas.csv'
        print(files_given_sim[0:10])
        ofile = hb.concatenate(b, files_given_sim, branching_factor=2)
        b.write_output(ofile, f'gs://sqtl_gtex/files/testing/mixture_testing/outputs_simulations_new_pipeline/all_{new_file_name}') 
b.run()











data_directory='/Users/hannahjacobs/Dropbox (MIT)/GradSchool/Finucane/splicing_variation_in_humans_2022/data/00_data/emp_bayes_input/'


alt_splice_sites=pd.read_csv(data_directory+'alt_splice_sites_filtered_for_popPSI.csv.gz', compression='gzip')





gtex_tissue_list=list(set(alt_splice_sites.tissue))


b = hb.Batch(backend=backend, name='emp_bayes')
num_jobs = 1000

#list_of_tissues=['Whole_Blood','Adipose_Subcutaneous']

list_of_tissues=gtex_tissue_list

b = hb.Batch(backend=backend, name='emp_bayes')

for tissue in list_of_tissues:
    
    
  
    for job_index in range(num_jobs):
    
        j = b.new_job(name='run_emp_bayes_'+tissue)
        j.memory('20Gi')
       
        #localizing the files
        
        j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
            
        input_script = "/home/run_optimizer_alt_ss_w_mixed_effects.py"

        input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')
    
        input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/alt_ss_for_EF_algorithm.csv.gz")
        
        j.command(f'python3 {input_script} \
                            --tissue {tissue} \
                                --filter_file {input_file_to_filter_events} \
                                --tissue_file {input_file_tissue} \
                                --output_file {j.output} \
                                --num_jobs {num_jobs} \
                                --job_index {job_index}')
        
        b.write_output(j.output, 'gs://sqtl_gtex/files/testing/mixture_testing/real_data_outputs/' + tissue + '_alt_ss_EF_5_FPR_5_' + str(job_index) + '_alphas_betas.csv')

      
b.run() 
    









skipped_exons_w_gene_names_and_assigned_abc_introns=pd.read_csv(data_dir+'00_data/skipped_exons_w_gene_names_and_assigned_abc_introns.csv.gz', compression='gzip',low_memory=False)











num_jobs = 1000

b = hb.Batch(backend=backend, name='emp_bayes')

for tissue in gtex_tissue_list:
    
    
    for job_index in range(num_jobs):
        
        j = b.new_job(name='run_emp_bayes_'+tissue)
        j.memory('20Gi')
           
        #localizing the files
            
        j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
        #if job_index<500:
        input_script = "/home/run_optimizer_SE_w_mixed_effects_unskipped_1.py"

        input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')
        
        input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/skipped_exons_w_gene_names_and_assigned_abc_introns.csv.gz")
           
        j.command(f'python3 {input_script} \
                                --tissue {tissue} \
                                    --filter_file {input_file_to_filter_events} \
                                    --tissue_file {input_file_tissue} \
                                    --output_file {j.output} \
                                    --num_jobs {num_jobs} \
                                    --job_index {job_index}')
            
        b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE/optimizer_results/final/' + tissue + '_SE_EF_5_FPR_5_' + str(job_index) +'_alphas_betas.csv')

    
b.run() 
    





b = hb.Batch(backend=backend, name='assign_EF')


    
for tissue in gtex_tissue_list:    
    j = b.new_job(name='run_assign_EF'+tissue)
    j.memory('20Gi')
    input_script = "/home/assign_EF.py"


    input_directory="gs://sqtl_gtex/files/EF_outputs/mixture_model/all_params_fitted/"
    j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')

    j.command(f'python3 {input_script} \
                --input_directory {input_directory} \
                --tissue_name {tissue} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/EFs_assigned/'+tissue+'_alt_ss_and_SEs_EF_5_FPR_5_values.csv.gz')

b.run() 
    












