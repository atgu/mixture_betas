import hail

import hailtop.batch as hb

backend = hb.ServiceBackend(billing_project = ##,
                            remote_tmpdir =##)

import pandas as pd

















b = hb.Batch(backend=backend, name='emp_bayes')
num_jobs = 25 #samples every 100 index



#scale to all lists of simulations
simulation_type=['single_1',
                'single_2',
                'double_1','double_2',
                 'triple_1', 'triple_2']
data_amt=['low', 'mid', 'high']
i=0
k=0
for i in range(len(simulation_type)):

    for k in range(len(data_amt)):
        l = b.new_job(name='all_data_ammounts')
        l.memory('20Gi')
        l.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v3')
        tissue='test_tissue'
    #localizing the files
        input_script = "/home/run_optimizer_alt_ss_w_mixed_effects.py"    
        
        input_file_tissue = b.read_input("gs://sqtl_gtex/files/testing/mixture_testing/inputs_simulations/"+data_amt[k]+"_NVE_"+simulation_type[i]+"_counts.csv.gz")
        
        input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/testing/mixture_testing/inputs_simulations/filtering_exons_in_test_sample.csv.gz")
            
        
        for job_index in range(num_jobs):
            
            j = b.new_job(name='run_emp_bayes_test_')
            j.memory('20Gi')
            j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v3')
        
            j.command(f'python3 {input_script} \
                            --tissue {tissue} \
                                --filter_file {input_file_to_filter_events} \
                                --tissue_file {input_file_tissue} \
                                --output_file {j.output} \
                                --num_jobs {num_jobs} \
                                --job_index {job_index}')
        
            b.write_output(j.output, f'gs://sqtl_gtex/files/testing/mixture_testing/outputs_simulations/{data_amt[k]}_{simulation_type[i]}_EF_FPR_5_alphas_betas-{job_index}.csv')
        
            
            
        b.run()



    





b = hb.Batch(backend=backend, name='emp_bayes')
num_jobs = 5 #samples every 100 index

j = b.new_job(name='run_emp_bayes_test')
j.memory('20Gi')
tissue='test_tissue'

#scale to all lists of simulations
simulation_type=['three_dist_exon_like_CAST']   
data_amt=['low', ']
i=0
k=0
for j in range(len(simulation_type)):
    for k in range(len(data_amt)):
    
    
        #localizing the files
        input_script = "/home/run_optimizer_alt_ss_w_mixed_effects.py"
        
        input_file_tissue = b.read_input("gs://sqtl_gtex/files/testing/mixture_testing/testing_for_pipeline_"+data_amt[k]+"_sample_exon_"+simulation_type[i]+".csv.gz")
        
        input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/testing/mixture_testing/filtering_exons_in_test_sample.csv.gz")
            
        j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v3')
        
        for job_index in range(num_jobs):
            j.command(f'python3 {input_script} \
                            --tissue {tissue} \
                                --filter_file {input_file_to_filter_events} \
                                --tissue_file {input_file_tissue} \
                                --output_file {j.output} \
                                --num_jobs {num_jobs} \
                                --job_index {job_index}')
        
            b.write_output(j.output, f'gs://sqtl_gtex/files/testing/mixture_testing/{data_amt[k]}_{simulation_type[i]}_EF_FPR_5_alphas_betas-{job_index}.csv')
        
            
            
        b.run()


b = hb.Batch(backend=backend, name='emp_bayes')
num_jobs = 1 #samples every 100 index

j = b.new_job(name='run_emp_bayes_test_high_sampled')
j.memory('20Gi')
tissue='test_tissue'

#gs://sqtl_gtex/files/testing/mixture_testing/testing_for_pipeline_low_sample_exon.csv.gz
   
    #localizing the files
input_script = "/home/run_optimizer_alt_ss_w_mixed_effects.py"

input_file_tissue = b.read_input("gs://sqtl_gtex/files/testing/mixture_testing/testing_for_pipeline_high_sample_exon.csv.gz")

input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/testing/mixture_testing/filtering_exons_in_test_sample.csv.gz")

j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v3')

for job_index in range(num_jobs):
    j.command(f'python3 {input_script} \
                    --tissue {tissue} \
                        --filter_file {input_file_to_filter_events} \
                        --tissue_file {input_file_tissue} \
                        --output_file {j.output} \
                        --num_jobs {num_jobs} \
                        --job_index {job_index}')

    b.write_output(j.output, f'gs://sqtl_gtex/files/testing/mixture_testing/high_sample_alt_ss_EF_5_FPR_5_alphas_betas-{job_index}.csv')
    break
b.run()





output_directory='/Users/hannahjacobs/MIT Dropbox/Hannah Jacobs/GradSchool/Finucane/splicing_variation_in_humans_2022/scripts/01_analysis_pipeline/00_run_emprical_bayes/emp_bayes_in_hail_pipeline-testing/data/mixture_implementation_testing/'

import pandas as pd


pd.read_csv(output_directory+'Whole_Blood_alt_ss_EF_5_FPR_5_alphas_betas-0_100_iters.csv').params.iloc[0


cluster_name=pd.read_csv(output_directory+'Whole_Blood_alt_ss_EF_5_FPR_5_alphas_betas-0_100_iters.csv').cluster_name.iloc[0]
#intron_2=pd.read_csv(output_directory+'Whole_Blood_alt_ss_EF_5_FPR_5_alphas_betas-0_300_iters.csv').intron_2.iloc[0]


pd.read_csv(output_directory+'Whole_Blood_alt_ss_EF_5_FPR_5_alphas_betas-0_100_iters.csv').params.iloc[0]


#pd.read_csv(output_directory+'Whole_Blood_alt_ss_EF_5_FPR_5_alphas_betas-0_300_iters_again.csv')




whole_blood = pd.read_csv('/Users/hannahjacobs/MIT Dropbox/Hannah Jacobs/GradSchool/Finucane/splicing_variation_in_humans_2022/GTEX sQTLs/GTEx_Analysis_v8_sQTL_leafcutter_counts/Whole_Blood_perind_numers.counts.gz',
                         low_memory=False,compression='gzip',
                          sep=' ', index_col=0)




cluster_name


k=whole_blood[whole_blood.index.isin(['chr10:114160266:114162637:clu_9616'])].values

N_minus_k=whole_blood[whole_blood.index.isin(['chr10:114157680:114162637:clu_9616'])].values


n=N_minus_k+k





pd.DataFrame({'k':k[0], 'n': n[0]}).to_csv(output_directory+'K_N_of_confusing_exon.csv')


import seaborn as sns


psi=(k[0][n[0]>0])/(n[0][n[0]>0])


sns.histplot(psi, bins=100)


#pd.read_csv(output_directory+'Whole_Blood_alt_ss_EF_5_FPR_5_alphas_betas-0_300_iters_again.csv')


list_of_LLs=pd.read_csv(output_directory+'Whole_Blood_alt_ss_EF_5_FPR_5_alphas_betas-0.csv').LogLikelihood.iloc[0]






pd.read_csv(output_directory+'Whole_Blood_alt_ss_EF_5_FPR_5_alphas_betas-0.csv').params_mixture.iloc[0]


#pd.read_csv(output_directory+'Whole_Blood_alt_ss_EF_5_FPR_5_alphas_betas-0.csv').function_output_emp_bayes.iloc[0]


pd.read_csv(output_directory+'Whole_Blood_alt_ss_EF_5_FPR_5_alphas_betas-0.csv')





b = hb.Batch(backend=backend, name='emp_bayes_alt_ss')
for tissue in gtex_tissue_list:
    
    
    
    j = b.new_job(name='run_emp_bayes_'+tissue)
    j.memory('10Gi')
   
    #localizing the files
    

    
    
        
    input_script = "/home/run_optimizer_alt_ss.py"

    input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')

    input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/alt_ss_for_EF_algorithm.csv.gz")
    
    j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v1')

    j.command(f'python3 {input_script} \
                --tissue {tissue} \
                --filter_file {input_file_to_filter_events} \
                --tissue_file {input_file_tissue} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/testing/EF_ouputs/alt_ss/optimizer_results/removed_prior/'+tissue+'_alt_ss_EF_5_FPR_5_alphas_betas.csv')

b.run() 


b = hb.Batch(backend=backend, name='assign_EF_alt_ss')
for tissue in gtex_tissue_list:
    
    j = b.new_job(name='assign_EF_'+tissue)
    j.memory('7Gi')
   
    #localizing the files
    
    input_script = "/home/assign_EF.py"

    input_file_tissue = b.read_input("gs://sqtl_gtex/files/EF_outputs/alt_ss/optimizer_results/"+tissue+"_alt_ss_alphas_betas.csv")

    j.image('gcr.io/finucane-splicing-hj/python-11-image-hj:v1')

    j.command(f'python3 {input_script} \
                --tissue_file {input_file_tissue} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/alt_ss/assigned_EFs/'+tissue+'_EFs_alt_ss.csv')

b.run() 


b = hb.Batch(backend=backend, name='concat_EFs')
for tissue in gtex_tissue_list:
    
    j = b.new_job(name='concat_EFs'+tissue)
    j.memory('7Gi')
   
    #localizing the files
    

    input_file_tissue = b.read_input("gs://sqtl_gtex/files/EF_outputs/alt_ss/assigned_EFs/"+tissue+"_EFs_alt_ss.csv")

    j.image('gcr.io/finucane-splicing-hj/python-11-image-hj:v1')

    j.command(f'python3 dfs = list() \
            for i, f in enumerate{tissue}: \
                data = pd.read_csv({tissue_file}) \
                data=data.assign(tissue={tissue}) \
                dfs.append(data)

                df = pd.concat(dfs, ignore_index=True)
                
                df.to_csv({j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/alt_ss/all_splicing_events.csv.gz')




b = hb.Batch(backend=backend, name='assign_EF_SE')
for tissue in gtex_tissue_list:
    
    j = b.new_job(name='assign_EF_'+tissue)
    j.memory('10Gi')
   
    #localizing the files
    
    input_script = "/home/assign_EF.py"

    input_file_tissue = b.read_input("gs://sqtl_gtex/files/EF_outputs/skipped_exons/optimizer_results/"+tissue+"_SE_alphas_betas.csv")

    j.image('gcr.io/finucane-splicing-hj/python-11-image-hj:v1')

    j.command(f'python3 {input_script} \
                --tissue_file {input_file_tissue} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/skipped_exons/assigned_EFs/'+tissue+'_EFs_SE.csv')

b.run() 








b = hb.Batch(backend=backend, name='emp_bayes')
for tissue in ['Whole_Blood']:
    
    
    
    j = b.new_job(name='run_emp_bayes_'+tissue)
    j.memory('7Gi')
   
    #localizing the files
    

    
    
    
    
    input_script = "/home/run_EF.py"

    input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')

    input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/alt_ss_for_EF_algorithm.csv.gz")
    
    j.image('gcr.io/finucane-splicing-hj/python-11-image-hj:v1')

    j.command(f'python3 {input_script} \
                --tissue {tissue} \
                --filter_file {input_file_to_filter_events} \
                --tissue_file {input_file_tissue} \
                --lower_bound {5} \
                --FPR {0.05} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/alt_ss/EF_5_percent_FPR_5_percent/'+tissue+'_alt_ss_EF_5_FPR_5_values.csv')

b.run() 
    





data_directory='/Users/hnjacobs/Dropbox (MIT)/GradSchool/Finuance/splicing_variation_in_humans_2022/data/00_data/emp_bayes_input/'


alt_splice_sites=pd.read_csv(data_directory+'alt_splice_sites_filtered_for_popPSI.csv.gz', compression='gzip')





gtex_tissue_list=list(set(alt_splice_sites.tissue))





#psi_directory='/Users/hnjacobs/Dropbox (MIT)/GradSchool/Finuance/GTEx sQTLs/GTEx_Analysis_v8_sQTL_leafcutter_counts/'









b = hb.Batch(backend=backend, name='emp_bayes')
for tissue in gtex_tissue_list:
    
    
    
    j = b.new_job(name='run_emp_bayes_'+tissue)
    j.memory('7Gi')
   
    #localizing the files
    

    
    
    
    
    input_script = "/home/run_EF.py"

    input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')

    input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/alt_ss_for_EF_algorithm.csv.gz")
    
    j.image('gcr.io/finucane-splicing-hj/python-11-image-hj:v1')

    j.command(f'python3 {input_script} \
                --tissue {tissue} \
                --filter_file {input_file_to_filter_events} \
                --tissue_file {input_file_tissue} \
                --lower_bound {0.05} \
                --FPR {0.05} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/alt_ss/EF_5_percent_FPR_5_percent/'+tissue+'_alt_ss_EF_5_FPR_5_values.csv')

b.run() 
    


b = hb.Batch(backend=backend, name='emp_bayes')
for tissue in gtex_tissue_list:
    
    
    
    j = b.new_job(name='run_emp_bayes_'+tissue)
    j.memory('7Gi')
   
    #localizing the files
    

    
    
    
    
    input_script = "/home/run_EF.py"

    input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')

    input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/alt_ss_for_EF_algorithm.csv.gz")
    
    j.image('gcr.io/finucane-splicing-hj/python-11-image-hj:v1')

    j.command(f'python3 {input_script} \
                --tissue {tissue} \
                --filter_file {input_file_to_filter_events} \
                --tissue_file {input_file_tissue} \
                --lower_bound {0.20} \
                --FPR {0.05} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/alt_ss/EF_20_percent/'+tissue+'_alt_ss_EF_20_FPR_5_values.csv')

b.run() 
    


python3 run_EF.py \
                --tissue 'Adipose_Subcutaneous' \
                --tissue_file '/Users/hnjacobs/Dropbox (MIT)/GradSchool/Finuance/GTEX sQTLs/GTEx_Analysis_v8_sQTL_leafcutter_counts/Adipose_Subcutaneous_perind_numers.counts.gz' \
                --filter_file '/Users/hnjacobs/Dropbox (MIT)//GradSchool/Finuance/splicing_variation_in_humans_2022/data/alt_splice_sites_filtered_for_popPSI.csv.gz' \
                --output_file Adipose_output.csv.gz
    
    






b = (backend=backend, name='emp_bayes')
for tissue in ['Adipose_Subcutaneous']:
    
    
    
    j = b.new_job(name='run_emp_bayes_'+tissue)
    j.memory('7Gi')
   
    #localizing the files
    

    
    
    
    
    input_script = "/home/run_EF.py"

    input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')

    input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/alt_splice_sites_filtered_for_popPSI.csv.gz")

    j.image('gcr.io/finucane-splicing-hj/python-11-image-hj:v1')

    j.command(f'python3 {input_script} \
                --tissue {tissue} \
                --filter_file {input_file_to_filter_events} \
                --tissue_file {input_file_tissue} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/'+tissue+'_EF_values.csv')

b.run() 
    









