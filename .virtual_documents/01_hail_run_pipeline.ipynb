import hail

import hailtop.batch as hb



import pandas as pd










alt_splice_sites=pd.read_csv(data_directory+'alt_splice_sites_filtered_for_popPSI.csv.gz', compression='gzip')





gtex_tissue_list=sorted(list(set(alt_splice_sites.tissue)))








b = hb.Batch(backend=backend, name='emp_bayes')
num_jobs = 25 #samples every 100 index



#scale to all lists of simulations
simulation_type=['single_1',
                'single_2',
                'double_1','double_2',
                 'triple_1', 'triple_2']
data_amt=['low', 'mid', 'high']
i=0
k=0
for i in range(len(simulation_type)):

    for k in range(len(data_amt)):
        l = b.new_job(name='all_data_ammounts')
        l.memory('20Gi')
        l.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
        tissue='test_tissue'
    #localizing the files
        input_script = "/home/run_optimizer_alt_ss_w_mixed_effects.py"    
        
        input_file_tissue = b.read_input("gs://sqtl_gtex/files/testing/mixture_testing/inputs_simulations/"+data_amt[k]+"_NVE_"+simulation_type[i]+"_counts.csv.gz")
        
        input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/testing/mixture_testing/inputs_simulations/filtering_exons_in_test_sample.csv.gz")
            
        
        for job_index in range(num_jobs):
            
            j = b.new_job(name='run_emp_bayes_test_')
            j.memory('20Gi')
            j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
        
            j.command(f'python3 {input_script} \
                            --tissue {tissue} \
                                --filter_file {input_file_to_filter_events} \
                                --tissue_file {input_file_tissue} \
                                --output_file {j.output} \
                                --num_jobs {num_jobs} \
                                --job_index {job_index}')
        
            b.write_output(j.output, f'gs://sqtl_gtex/files/testing/mixture_testing/outputs_simulations_new_pipeline/{data_amt[k]}_{simulation_type[i]}_EF_FPR_5_alphas_betas-{job_index}.csv')
        
            
b.run()



    


b = hb.Batch(backend=backend, name='emp_bayes')
directory='gs://sqtl_gtex/files/testing/mixture_testing/outputs_simulations_new_pipeline/'

j = b.new_job(name='run_emp_bayes_test_')
j.memory('20Gi')
j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
for i in range(len(simulation_type)):
    for k in range(len(data_amt)):
        files_given_sim=[]
        
        for job_index in range(num_jobs):
    
            file=data_amt[k]+'_'+simulation_type[i]+'_EF_FPR_5_alphas_betas-'+str(job_index)+'.csv'
            
            o_file=b.read_input(directory+file)

            files_given_sim.append(o_file)
            new_file_name=data_amt[k]+'_'+simulation_type[i]+'_EF_FPR_5_alphas_betas.csv'
        print(files_given_sim[0:10])
        ofile = hb.concatenate(b, files_given_sim, branching_factor=2)
        b.write_output(ofile, f'gs://sqtl_gtex/files/testing/mixture_testing/outputs_simulations_new_pipeline/all_{new_file_name}') 
b.run()











data_directory='/Users/hannahjacobs/Dropbox (MIT)/GradSchool/Finucane/splicing_variation_in_humans_2022/data/00_data/emp_bayes_input/'


alt_splice_sites=pd.read_csv(data_directory+'alt_splice_sites_filtered_for_popPSI.csv.gz', compression='gzip')





gtex_tissue_list=list(set(alt_splice_sites.tissue))


b = hb.Batch(backend=backend, name='emp_bayes')
num_jobs = 1000

#list_of_tissues=['Whole_Blood','Adipose_Subcutaneous']

list_of_tissues=gtex_tissue_list

b = hb.Batch(backend=backend, name='emp_bayes')

for tissue in list_of_tissues:
    
    
  
    for job_index in range(num_jobs):
    
        j = b.new_job(name='run_emp_bayes_'+tissue)
        j.memory('20Gi')
       
        #localizing the files
        
        j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
            
        input_script = "/home/run_optimizer_alt_ss_w_mixed_effects.py"

        input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')
    
        input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/alt_ss_for_EF_algorithm.csv.gz")
        
        j.command(f'python3 {input_script} \
                            --tissue {tissue} \
                                --filter_file {input_file_to_filter_events} \
                                --tissue_file {input_file_tissue} \
                                --output_file {j.output} \
                                --num_jobs {num_jobs} \
                                --job_index {job_index}')
        
        b.write_output(j.output, 'gs://sqtl_gtex/files/testing/mixture_testing/real_data_outputs/' + tissue + '_alt_ss_EF_5_FPR_5_' + str(job_index) + '_alphas_betas.csv')

      
b.run() 
    









directory='gs://sqtl_gtex/files/testing/mixture_testing/real_data_outputs/'
num_jobs=1000


list_of_tissues=gtex_tissue_list
directory='gs://sqtl_gtex/files/EF_outputs/mixture_model/SE/optimizer_results/'
for tissue in gtex_tissue_list[0:2]:
    b = hb.Batch(backend=backend, name='emp_bayes'+tissue)
    
    files_given_sim=[]
    new_file_name = tissue+'_EF_FPR_5_alphas_betas.csv'
    for job_index in range(10):
        
        
        inner_j = b.new_job(name='run_emp_bayes_test_'+tissue+'load_tissues')
        inner_j.memory('20Gi')
        inner_j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
        
        file=tissue + '_alt_ss_EF_5_FPR_5_' + str(job_index) + '_alphas_betas.csv'
            
        o_file=b.read_input(directory+file)
        inner_j.output=o_file
        files_given_sim.append(o_file)
        
        
    #j = b.new_job(name='run_emp_bayes_test_'+tissue+'concat_files_w_params')
    #j.memory('20Gi')
    #j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
    #once done, run all files

    ofile = hb.concatenate(b, files_given_sim, branching_factor=2,image='us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4' )
    
    b.write_output(ofile, f'gs://sqtl_gtex/files/testing/mixture_testing/real_data_outputs/all_params_fitted/all_{new_file_name}') 
    b.run()


files_given_sim


b = Batch()
j1 = b.new_job()
j1.command(f'touch {j1.ofile}')
j2 = b.new_job()
j2.command(f'touch {j2.ofile}')
j3 = b.new_job()
j3.command(f'touch {j3.ofile}')
files = [j1.ofile, j2.ofile, j3.ofile]
ofile = concatenate(b, files, branching_factor=2)
b.run()





data_dir='/Users/hannahjacobs/MIT Dropbox/Hannah Jacobs/GradSchool/Finucane/splicing_variation_in_humans_2022/data/'


skipped_exons_w_gene_names_and_assigned_abc_introns=pd.read_csv(data_dir+'00_data/skipped_exons_w_gene_names_and_assigned_abc_introns.csv.gz', compression='gzip',low_memory=False)








b = hb.Batch(backend=backend, name='emp_bayes')
num_jobs = 1000

#list_of_tissues=['Whole_Blood','Adipose_Subcutaneous']

list_of_tissues=['Whole_Blood','Esophagus_Gastroesophageal_Junction']

b = hb.Batch(backend=backend, name='emp_bayes')

for tissue in list_of_tissues:
    #range(num_jobs)
    for job_index in range(100):
    
        j = b.new_job(name='run_emp_bayes_'+tissue)
        j.memory('20Gi')
       
        #localizing the files
        
        j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
            
        input_script = "/home/run_optimizer_SE_w_mixed_effects.py"
    
        input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')
    
        input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/skipped_exons_w_gene_names_and_assigned_abc_introns.csv.gz")
       
        j.command(f'python3 {input_script} \
                            --tissue {tissue} \
                                --filter_file {input_file_to_filter_events} \
                                --tissue_file {input_file_tissue} \
                                --output_file {j.output} \
                                --num_jobs {num_jobs} \
                                --job_index {job_index}')
        #/sqtl_gtex/files/EF_outputs/mixture_model/SE/optimizer_results
        b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE/optimizer_results/' + tissue + '_SE_EF_5_FPR_5_' + str(job_index) +'_alphas_betas.csv')
    
b.run()
    





num_jobs = 1000

b = hb.Batch(backend=backend, name='emp_bayes')

for tissue in gtex_tissue_list:
    
    
    for job_index in range(num_jobs):
        
        j = b.new_job(name='run_emp_bayes_'+tissue)
        j.memory('20Gi')
           
        #localizing the files
            
        j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
        #if job_index<500:
        input_script = "/home/run_optimizer_SE_w_mixed_effects_unskipped_1.py"

        input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')
        
        input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/skipped_exons_w_gene_names_and_assigned_abc_introns.csv.gz")
           
        j.command(f'python3 {input_script} \
                                --tissue {tissue} \
                                    --filter_file {input_file_to_filter_events} \
                                    --tissue_file {input_file_tissue} \
                                    --output_file {j.output} \
                                    --num_jobs {num_jobs} \
                                    --job_index {job_index}')
            
        b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE/optimizer_results/final/' + tissue + '_SE_EF_5_FPR_5_' + str(job_index) +'_alphas_betas.csv')

    
b.run() 
    


#b = hb.Batch(backend=backend, name='emp_bayes')
#j = b.new_job(name='concat_run'+tissue)
#j.memory('20Gi')
#j.command(f'gsutil cat 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE/optimizer_results/new/'+{tissue}+'_SE_EF_5_FPR_5_'+*+'.csv'+>+'gs://sqtl_gtex/files/testing/mixture_testing/real_data_outputs/all_params_fitted/all_EF_SEs_alphas_and_betas_from_mixture_'+{tissue}+'.csv')


#b.run() 





b = hb.Batch(backend=backend, name='assign_EF')


    
for tissue in ['Vagina','Whole_Blood']:    
    j = b.new_job(name='run_assign_cluster_names_and_exclusion_type_SEs_'+tissue)
       
    j.memory('20Gi')
           
            #localizing the files
            
    j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
                
    input_script = "/home/assign_cluster_names.py"
        
    input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')
        
    input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/skipped_exons_w_gene_names_and_assigned_abc_introns.csv.gz")
    j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')

    j.command(f'python3 {input_script} \
                                --tissue {tissue} \
                                    --filter_file {input_file_to_filter_events} \
                                    --tissue_file {input_file_tissue} \
                                    --output_file {j.output} \
                                    --counts_used_file {j.f_1} \
                                     --skipped_1_file {j.f_2} \
                                     --skipped_2_file {j.f_3}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE_info/'+tissue+'_SE_extra_info.csv')

    b.write_output(j.f_1, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE_info/'+tissue+'_counts_used_counts.csv')

    b.write_output(j.f_2, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE_info/'+tissue+'_skipped_1_coounts.csv')

    b.write_output(j.f_3, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE_info/'+tissue+'_skipped_2_counts.csv')

b.run() 
    


b = hb.Batch(backend=backend, name='assign_EF')


    
for tissue in gtex_tissue_list:    
    j = b.new_job(name='run_assign_cluster_names_and_exclusion_type_SEs_'+tissue)
       
    j.memory('20Gi')
           
            #localizing the files
            
    j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')
                
    input_script = "/home/assign_cluster_names.py"
        
    input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')
        
    input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/skipped_exons_w_gene_names_and_assigned_abc_introns.csv.gz")
    j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')

    j.command(f'python3 {input_script} \
                                --tissue {tissue} \
                                    --filter_file {input_file_to_filter_events} \
                                    --tissue_file {input_file_tissue} \
                                    --output_file {j.output} \
                                    --counts_used_file {j.f_1} \
                                     --skipped_1_file {j.f_2} \
                                     --skipped_2_file {j.f_3}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE_info/'+tissue+'_SE_extra_info.csv')

    b.write_output(j.f_1, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE_info/'+tissue+'_counts_used_counts.csv')

    b.write_output(j.f_2, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE_info/'+tissue+'_skipped_1_coounts.csv')

    b.write_output(j.f_3, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/SE_info/'+tissue+'_skipped_2_counts.csv')

b.run() 
    


b = hb.Batch(backend=backend, name='assign_EF')


    
for tissue in ['Esophagus_Mucosa']:    
    j = b.new_job(name='run_assign_EF'+tissue)
    j.memory('20Gi')
    input_script = "/home/assign_EF.py"

#gs://sqtl_gtex/files/testing/mixture_testing/real_data_outputs/
   # input_directory="gs://sqtl_gtex/files/EF_outputs/mixture_model/all_params_fitted/param_values_final/"
    input_directory="gs://sqtl_gtex/files/testing/mixture_testing/real_data_outputs/all_params_fitted/"
    j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')

    j.command(f'python3 {input_script} \
                --input_directory {input_directory} \
                --tissue_name {tissue} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/EFs_assigned/all_SE_params_'+tissue+'.csv.gz')

b.run() 
    


b = hb.Batch(backend=backend, name='assign_EF')

    #localizing the files
    

    
for tissue in gtex_tissue_list:    
    j = b.new_job(name='run_assign_EF'+tissue)
    j.memory('20Gi')
    input_script = "/home/assign_EF.py"

#gs://sqtl_gtex/files/testing/mixture_testing/real_data_outputs/
   # input_directory="gs://sqtl_gtex/files/EF_outputs/mixture_model/all_params_fitted/param_values_final/"
    input_directory="gs://sqtl_gtex/files/testing/mixture_testing/real_data_outputs/all_params_fitted/"
    j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')

    j.command(f'python3 {input_script} \
                --input_directory {input_directory} \
                --tissue_name {tissue} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/EFs_assigned/'+tissue+'_alt_ss_and_SEs_EF_5_FPR_5_values.csv.gz')

b.run() 
    





b = hb.Batch(backend=backend, name='assign_EF')


    
for tissue in gtex_tissue_list:    
    j = b.new_job(name='run_assign_EF'+tissue)
    j.memory('20Gi')
    input_script = "/home/assign_EF.py"


    input_directory="gs://sqtl_gtex/files/EF_outputs/mixture_model/all_params_fitted/"
    j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')

    j.command(f'python3 {input_script} \
                --input_directory {input_directory} \
                --tissue_name {tissue} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/EFs_assigned/'+tissue+'_alt_ss_and_SEs_EF_5_FPR_5_values.csv.gz')

b.run() 
    


b = hb.Batch(backend=backend, name='assign_EF')


    
for tissue in gtex_tissue_list:    
    j = b.new_job(name='run_assign_EF'+tissue)
    j.memory('20Gi')
    input_script = "/home/assign_EF.py"


    input_directory="gs://sqtl_gtex/files/EF_outputs/mixture_model/all_params_fitted/"
    j.image('us-central1-docker.pkg.dev/finucane-splicing-hj/testing/testing_beta_binom:v4')

    j.command(f'python3 {input_script} \
                --input_directory {input_directory} \
                --tissue_name {tissue} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/mixture_model/EFs_assigned/'+tissue+'_alt_ss_and_SEs_EF_5_FPR_5_values.csv.gz')

b.run() 











b = hb.Batch(backend=backend, name='emp_bayes')
for tissue in gtex_tissue_list:
    
    
    
    j = b.new_job(name='run_emp_bayes_'+tissue)
    j.memory('7Gi')
   
    #localizing the files
    

    
    
    
    
    input_script = "/home/run_EF.py"

    input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')

    input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/alt_ss_for_EF_algorithm.csv.gz")
    
    j.image('gcr.io/finucane-splicing-hj/python-11-image-hj:v1')

    j.command(f'python3 {input_script} \
                --tissue {tissue} \
                --filter_file {input_file_to_filter_events} \
                --tissue_file {input_file_tissue} \
                --lower_bound {0.20} \
                --FPR {0.05} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/alt_ss/EF_20_percent/'+tissue+'_alt_ss_EF_20_FPR_5_values.csv')

b.run() 
    


python3 run_EF.py \
                --tissue 'Adipose_Subcutaneous' \
                --tissue_file '/Users/hnjacobs/Dropbox (MIT)/GradSchool/Finuance/GTEX sQTLs/GTEx_Analysis_v8_sQTL_leafcutter_counts/Adipose_Subcutaneous_perind_numers.counts.gz' \
                --filter_file '/Users/hnjacobs/Dropbox (MIT)//GradSchool/Finuance/splicing_variation_in_humans_2022/data/alt_splice_sites_filtered_for_popPSI.csv.gz' \
                --output_file Adipose_output.csv.gz
    
    






b = (backend=backend, name='emp_bayes')
for tissue in ['Adipose_Subcutaneous']:
    
    
    
    j = b.new_job(name='run_emp_bayes_'+tissue)
    j.memory('7Gi')
   
    #localizing the files
    

    
    
    
    
    input_script = "/home/run_EF.py"

    input_file_tissue = b.read_input("gs://sqtl_gtex/exon_exon_counts_gtex/"+tissue+'_perind_numers.counts.gz')

    input_file_to_filter_events = b.read_input("gs://sqtl_gtex/files/EF_inputs/alt_splice_sites_filtered_for_popPSI.csv.gz")

    j.image('gcr.io/finucane-splicing-hj/python-11-image-hj:v1')

    j.command(f'python3 {input_script} \
                --tissue {tissue} \
                --filter_file {input_file_to_filter_events} \
                --tissue_file {input_file_tissue} \
                --output_file {j.output}')

    b.write_output(j.output, 'gs://sqtl_gtex/files/EF_outputs/'+tissue+'_EF_values.csv')

b.run() 
    









