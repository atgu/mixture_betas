#!/usr/bin/env python
# coding: utf-8
import numpy
import sys
from operator import itemgetter
import scipy
import numpy as np

import scipy.optimize as opt
from scipy import stats
from tqdm import tqdm
import pandas as pd
numpy.random.seed(1)
import argparse
from scipy.optimize import minimize
import diptest
from scipy.stats import chi2

# In[ ]:
#ALT SS

# In[288]:


# In[68]:

parser = argparse.ArgumentParser()
parser.add_argument("--tissue", type=str)
parser.add_argument("--tissue_file", type=str)
parser.add_argument("--filter_file", type=str)
parser.add_argument("-o", "--output_file", help="Directs the output to a name of your choice")
parser.add_argument("--num_jobs", type=int)
parser.add_argument("--job_index", type=int)

args = parser.parse_args()



# # Modelling PSI variation among individuals with hierarchical binomial models
# 
# Background
# --
# Consider a binary alternative splicing event, such as a skipped exon or a pair of alternative 3' or 5' splice sites. In both cases there is an exon segment that can either be included or skipped. In the case of the skipped exon, the entire exon is either included or excluded. In the case of the alternative splice sites, the segment of sequence between the two splice sites is either included or excluded. In each case, there are two possible isoforms: an inclusion form and an exclusion form.
# 
# How frequently the variable sequence is included rather than excluded is generally termed  𝜓  (PSI, or percent-spliced-in). If  𝜆𝐼  is the amount of inclusion-form generated by the cell and  𝜆𝐸  is the amount of exclusion-form, then  𝜓  is defined
# 
# 𝜓=𝜆𝐼𝜆𝐼+𝜆𝐸
#  
# But we never have access to those true rates. Instead, we generally try to estimate PSI from noisy sequencing data. In this setting, we usually have sequencing read counts supporting exon-exon junctions which belong to either the inclusion or exclusion forms. In the case of skipped exons, there are two junctions supporting the inclusion form, which needs to be accounted for when estimating PSI. Because this complicates matters slightly, we'll instead focus on alternative splice sites where both the inclusion and exclusion form are each supported by a single exon-exon junction.
# 
# Let's call the number of reads supporting the inclusion form  𝑟  and the number of reads supporting the exclusion form  𝑞 . The total number of reads supporting the binary splicing event is  𝑛=𝑟+𝑞 . We want to infer  𝜓  from the observed read counts  𝑟  and  𝑞 . A naive approach would to just compute the ratio
# 
# 𝜓̂ =𝑟𝑟+𝑞
#  
# This, however, has several undesirable quantities. The most disastrous is that it does not account for noisy count data can be. If  𝑟+𝑞  is small, this estimator  𝜓̂   is likely to vary considerably around the true  𝜓 . But a more subtle problem is that this estimator assumes that absence of evidence is evidence of absence. That is, it can estimate  𝜓̂   equals zero or one, and will tend to do so when  𝑟+𝑞  is small.

# In[289]:

#############just changes this from intron 1############
#a_2 = unskipped_2_counts_give_tissue[~unskipped_2_counts_give_tissue.minority_is_exclusion_event].sort_values(by='cluster_name', ascending=False)

#b_i = unskipped_2_counts_give_tissue[unskipped_2_counts_give_tissue.minority_is_exclusion_event].sort_values(by='cluster_name', ascending=False)



tissue=args.tissue
tissue_file=args.tissue_file
filter_file=args.filter_file



counts=pd.read_csv(tissue_file, compression='gzip', sep=' ', index_col=0)

skipped_exons=pd.read_csv(filter_file, compression='gzip')




#merged based on tissue and cluster ID
unskipped_1=skipped_exons[0::3]

skipped=skipped_exons[1::3]

unskipped_2=skipped_exons[2::3]

counts_w_ID = counts.reset_index().rename(columns={'index':'ID'})

counts_w_ID['tissue']=tissue

skipped_counts_give_tissue = skipped[['ID', 'tissue']].merge(counts_w_ID, on = ['ID', 'tissue'], how = 'inner')
    
unskipped_1_counts_give_tissue = unskipped_1[['ID', 'tissue']].merge(counts_w_ID, on = ['ID', 'tissue'], how = 'inner')

unskipped_2_counts_give_tissue = unskipped_2[['ID', 'tissue']].merge(counts_w_ID, on = ['ID', 'tissue'], how = 'inner')


skipped_raw = skipped_counts_give_tissue.loc[:, ~skipped_counts_give_tissue.columns.isin(['ID', 'tissue'])].values

unskipped_raw_1 = unskipped_1_counts_give_tissue.loc[:, ~unskipped_1_counts_give_tissue.columns.isin(['ID', 'tissue'])].values

unskipped_raw_2 = unskipped_2_counts_give_tissue.loc[:, ~unskipped_2_counts_give_tissue.columns.isin(['ID', 'tissue'])].values

#apply filter for difference of a and b reads
log_ratio=(np.log(np.mean(unskipped_raw_1, axis=1)) - np.log(np.mean(unskipped_raw_2, axis=1)))

#ROUNDED TO THREE DECIMALS?
filtered_exons = (np.abs(log_ratio) <= 1)

skipped_counts_give_tissue=skipped_counts_give_tissue[filtered_exons==True]
unskipped_1_counts_give_tissue=unskipped_1_counts_give_tissue[filtered_exons==True]
unskipped_2_counts_give_tissue=unskipped_2_counts_give_tissue[filtered_exons==True]


#figure out which event is lower: skipped or unskipped
total_reads_supporting_skipped =  skipped_counts_give_tissue.loc[:, ~skipped_counts_give_tissue.columns.isin(['ID', 'tissue'])].sum(axis=1)

total_reads_supporting_unskipped = unskipped_1_counts_give_tissue.loc[:, ~unskipped_1_counts_give_tissue.columns.isin(['ID', 'tissue'])].sum(axis=1)


#pick the exon with less total reads across all individuals 
skipped_is_lower_PSI = total_reads_supporting_skipped <= total_reads_supporting_unskipped

unskipped_1_counts_give_tissue['minority_is_exclusion_event'] = skipped_is_lower_PSI

skipped_counts_give_tissue['minority_is_exclusion_event'] = skipped_is_lower_PSI

unskipped_2_counts_give_tissue['minority_is_exclusion_event'] = skipped_is_lower_PSI

####assign cluster names
cluster_name = unskipped_1_counts_give_tissue.ID + '_' + skipped_counts_give_tissue.ID + '_' + unskipped_2_counts_give_tissue.ID.apply(lambda x: x.split(':clu')[0]) 

unskipped_1_counts_give_tissue=unskipped_1_counts_give_tissue.assign(cluster_name=cluster_name)
unskipped_2_counts_give_tissue=unskipped_2_counts_give_tissue.assign(cluster_name=cluster_name)
skipped_counts_give_tissue=skipped_counts_give_tissue.assign(cluster_name=cluster_name)


####assign to alphas anmd betas
a_i = skipped_counts_give_tissue[skipped_counts_give_tissue.minority_is_exclusion_event].sort_values(by='cluster_name', ascending=False)
#########just changes this############
a_2 = unskipped_2_counts_give_tissue[~unskipped_2_counts_give_tissue.minority_is_exclusion_event].sort_values(by='cluster_name', ascending=False)

b_i = unskipped_2_counts_give_tissue[unskipped_2_counts_give_tissue.minority_is_exclusion_event].sort_values(by='cluster_name', ascending=False)

b_2 = skipped_counts_give_tissue[~skipped_counts_give_tissue.minority_is_exclusion_event].sort_values(by='cluster_name', ascending=False)

#prepare for optimizer
a=pd.concat([a_i, a_2])
b=pd.concat([b_i, b_2])

cluster_names = a.cluster_name

skipped_exon_type = a.minority_is_exclusion_event

alpha_counts = a.drop(columns=(['ID', 'tissue', 'minority_is_exclusion_event','cluster_name']))

beta_counts = b.drop(columns=(['ID', 'tissue', 'minority_is_exclusion_event','cluster_name']))

alpha_counts.index = cluster_names
################### algorthm as same as alt ss now


def LR_test(LR, df=1):
    return chi2.sf(-2 * LR, df)

def compute_log_likelihood(k, n, params):
    # compute log( \prod_j \sum_i pi_i pmf(k_j, n_j, a, b) )
    n_observations = k.shape[0]
    n_components = params.shape[0] // 3

    LogL = 0
    for i in range(n_components):
        a, b, pi = params[i], params[i + n_components], params[i + 2*n_components]
        #LogL += np.log(pi) + logsumexp( scipy.stats.betabinom.logpmf(k, n=n, a=a, b=b) )
        LogL += np.log(pi) +  scipy.stats.betabinom.logpmf(k, n=n, a=a, b=b).sum() 
    return LogL

def method_lbfgs(k, n, weights=None):
    obj_func_ = lambda x, k, n: -np.average(
                                  scipy.stats.betabinom.logpmf(
                                        #exon a reads
                                        k,
                                        #a+b reads
                                        n=n,
                                        #alpha
                                        a=2.**x[0],
                                        # beta
                                        b=2.**x[1]
                                  ), weights=weights)
    bounds = None

    obj_func = lambda x : np.arcsinh(obj_func_(x, k, n))

    x0=np.ones(2)
    minimized_a_b = minimize(obj_func, x0=x0, method='L-BFGS-B', bounds=bounds, options={'maxiter':100},
                            tol=1e-6) #change back to 100 when done debugging
    
    [a_est,b_est] = 2**minimized_a_b.x
    params = np.array([a_est, b_est, 1])
    return params, minimized_a_b

def method_lbfgs_em(k, n, n_components, em_iterations = 100):
    # params = [a1, a2, a3, b1, b2, b3, pi1, pi2, pi3]
    # a, b, pi = params[k], params[k + self.n_components], params[k + 2 * self.n_components]
    # initialize
    n_observations = k.shape[0]

    params = np.zeros(3*n_components)
    for i in range(n_components):
        params[i] = 1 + i
        params[i + n_components] = 1 + n_components - i
    params[2*n_components:] = 1/n_components
    gamma = np.zeros((n_components, n_observations))
    LogL = []
    minimized_a_b_function=[]
    if n_components == 1:
        em_iterations = 1
    for iter in range(em_iterations):
        # E step
        for i in range(n_components):
            a, b, pi = params[i], params[i + n_components], params[i + 2*n_components]
            gamma[i, :] = pi * scipy.stats.betabinom.pmf(k, n=n, a=a, b=b)
        gamma /= gamma.sum(axis=0)
        
        # M step
        params[2*n_components:3*n_components] = gamma.mean(axis=1)
        for i in range(n_components):
            weights = gamma[i, :]
            (params[i], params[i + n_components], _), obj_func = method_lbfgs(k, n, weights)
        minimized_a_b_function.append(obj_func)
        LogL.append(compute_log_likelihood(k, n, params))
    return params, LogL, minimized_a_b_function

# Function
# --

# In[324]:
def est_mixture_of_alphas_and_betas_w_restarts(exon_a, exon_b, n_components, em_iterations = 100):
        
        #drop any totally zero values in calculation
    a=exon_a[(exon_b+exon_a)>0]
    b=exon_b[(exon_b+exon_a)>0]

    psi = (a + 1) / (a + b + 2)
    n = a+b
    k = a

    for attempts in range(3):
        try:
            params, LogL, minimized_a_b_function = method_lbfgs_em(k, n, n_components, em_iterations)
            break
        except RuntimeWarning:
            continue
    else:
        params, LogL = None, None, None


       # break
    return params, LogL, minimized_a_b_function

def est_alphas_and_betas(counts_df_a, counts_df_b, counts_df_with_cluster_name_info, power_transform, arcsin_transform, num_jobs, job_index):
    i=0

    FPR=0.05


    
    output_of_min_funcs=[]
    sum_of_all_reads=[]
    est_alphas_and_betas_list=[]
    est_LL=[]

    a_raw=counts_df_a.values
    b_raw=counts_df_b.values
    
    dip_results_pval=[]
    
    final_cluster_names=[]
    est_LL_triple=[]
    est_LL_double=[]
    params_triple=[]
    params_double=[]
    p_values_for_diptest=[]
    p_values_for_LR_test=[]
    not_sucesses=[]   
    j=0
    
    for exon_a,exon_b in tqdm(zip(a_raw, b_raw)):
        
        i+=1
        
        if i % num_jobs != job_index:
            continue
        #final_cluster_names.append(cluster_names.iloc[i])
        
        final_cluster_names.append(counts_df_a.index[i])
        #drop any totally zero values in calculation
        a=exon_a[(exon_b+exon_a)>0]

        b=exon_b[(exon_b+exon_a)>0]
        # added puesdo count
        psi = (a + 1) / (a + b + 2)
        n=a+b
        sum_of_all_reads.append(n.sum())

        
        
        # perform the diptest across all PSI when there are are at least 1 count across both exons
        stat,pval=diptest.diptest(psi)
        dip_results_pval.append(pval)
        
        p_values_for_diptest.append(pval)

############################# 1. calculate empirical bayes using chosen method depending on diptest and LR ############################

        #intialize priors
        x0=np.ones(2)
        
        #minimize function of single component if and only if the diptest is not sigificant (BF corrected in tissue)
# minimize function of single component if and only if the diptest is not significant (BF corrected in tissue)
        #if pval >= 0.05/len(a_raw):
            #params, Log_Likelihood, minimized_a_b = est_mixture_of_alphas_and_betas_w_restarts(exon_a, exon_b, 1)
            #est_LL_triple.append(np.nan)
            #est_LL_double.append(np.nan)
            #params_triple.append(np.nan)
            #params_double.append(np.nan)
            #p_values_for_LR_test.append(np.nan)
            


        # perform the EM algorithm with 2 and 3 components if the diptest is significant (assumption of unimodality can be rejected), then perform LR test
        #else:
        double_params, Log_Likelihood_double, minimized_a_b_double = est_mixture_of_alphas_and_betas_w_restarts(exon_a, exon_b, 2)


        triple_params, Log_Likelihood_triple, minimized_a_b_triple = est_mixture_of_alphas_and_betas_w_restarts(exon_a, exon_b, 3)


        p_val_LR = LR_test(Log_Likelihood_triple[-1] - Log_Likelihood_double[-1], df = 3)
            #DOF is 4 for these two tests: if pvalue is larger than alpha/len(a_raw) - BF corrected
            
            #use double if LR is not significant
        if p_val_LR > 0.05/len(a_raw):
            #use double if LR is significant
            [Log_Likelihood, minimized_a_b, params]=[Log_Likelihood_double, minimized_a_b_double, double_params]
            #use triple is LR is significant
        elif  p_val_LR <= 0.05/len(a_raw):
            [Log_Likelihood, minimized_a_b, params]=[Log_Likelihood_triple, minimized_a_b_triple, triple_params]

        p_values_for_LR_test.append(p_val_LR)
        est_LL_triple.append(Log_Likelihood_triple)
        est_LL_double.append(Log_Likelihood_double)
        params_triple.append(triple_params)
        params_double.append(double_params)
        
        
        
        output_of_min_funcs.append(minimized_a_b)

        est_alphas_and_betas_list.append(params)
        
        est_LL.append(Log_Likelihood)


    return est_alphas_and_betas_list, output_of_min_funcs, final_cluster_names, est_LL, sum_of_all_reads,  est_LL_triple, est_LL_double, params_triple, params_double, p_values_for_LR_test, p_values_for_diptest
        

           



# run emp bayes to generate alphas and betas
num_jobs=args.num_jobs
job_index=args.job_index

#print(a)
#print(a.cluster_names.iloc[1])
[est_alphas_and_betas_list, output_of_min_funcs, final_cluster_names_for_df, est_LL, sum_of_all_reads, est_LL_triple, est_LL_double, params_triple, params_double, p_values_for_LR_test,p_values_for_diptest ] = est_alphas_and_betas(alpha_counts,beta_counts, a, True,True, num_jobs, job_index)

#print(len(est_alphas_and_betas_list))
#print(len(final_cluster_names_for_df))
output_df=pd.DataFrame( {
                'total_reads_spanning_all_junctions': sum_of_all_reads, 
                'function_output_emp_bayes':output_of_min_funcs,
                'number_people_in_sample':len(alpha_counts.columns),
                'params': est_alphas_and_betas_list,
                'cluster_name':final_cluster_names, 
                'LogLikelihood':est_LL,
                'LogLikelihood_triple':est_LL_triple,
                'LogLikelihood_double':est_LL_double,
                'params_triple':params_triple,
                'params_double':params_double,
                'pvalue_LR':p_values_for_LR_test,
                'pvalue_dip':p_values_for_diptest} )





#write output df
with open(args.output_file, 'w') as output_file:
    output_df.to_csv(output_file)
    



