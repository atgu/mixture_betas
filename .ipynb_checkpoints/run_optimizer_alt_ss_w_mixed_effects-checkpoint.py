#!/usr/bin/env python
# coding: utf-8
import numpy
import sys
from operator import itemgetter
import scipy
import numpy as np

import scipy.optimize as opt
from scipy import stats
from tqdm import tqdm
import pandas as pd
numpy.random.seed(1)
import argparse
from scipy.optimize import minimize

import diptest
# In[ ]:
#ALT SS

# In[288]:


# In[68]:

parser = argparse.ArgumentParser()
parser.add_argument("--tissue", type=str)
parser.add_argument("--tissue_file", type=str)
parser.add_argument("--filter_file", type=str)
parser.add_argument("-o", "--output_file", help="Directs the output to a name of your choice")
parser.add_argument("--num_jobs", type=int)
parser.add_argument("--job_index", type=int)

args = parser.parse_args()



# # Modelling PSI variation among individuals with hierarchical binomial models
# 
# Background
# --
# Consider a binary alternative splicing event, such as a skipped exon or a pair of alternative 3' or 5' splice sites. In both cases there is an exon segment that can either be included or skipped. In the case of the skipped exon, the entire exon is either included or excluded. In the case of the alternative splice sites, the segment of sequence between the two splice sites is either included or excluded. In each case, there are two possible isoforms: an inclusion form and an exclusion form.
# 
# How frequently the variable sequence is included rather than excluded is generally termed  𝜓  (PSI, or percent-spliced-in). If  𝜆𝐼  is the amount of inclusion-form generated by the cell and  𝜆𝐸  is the amount of exclusion-form, then  𝜓  is defined
# 
# 𝜓=𝜆𝐼𝜆𝐼+𝜆𝐸
#  
# But we never have access to those true rates. Instead, we generally try to estimate PSI from noisy sequencing data. In this setting, we usually have sequencing read counts supporting exon-exon junctions which belong to either the inclusion or exclusion forms. In the case of skipped exons, there are two junctions supporting the inclusion form, which needs to be accounted for when estimating PSI. Because this complicates matters slightly, we'll instead focus on alternative splice sites where both the inclusion and exclusion form are each supported by a single exon-exon junction.
# 
# Let's call the number of reads supporting the inclusion form  𝑟  and the number of reads supporting the exclusion form  𝑞 . The total number of reads supporting the binary splicing event is  𝑛=𝑟+𝑞 . We want to infer  𝜓  from the observed read counts  𝑟  and  𝑞 . A naive approach would to just compute the ratio
# 
# 𝜓̂ =𝑟𝑟+𝑞
#  
# This, however, has several undesirable quantities. The most disastrous is that it does not account for noisy count data can be. If  𝑟+𝑞  is small, this estimator  𝜓̂   is likely to vary considerably around the true  𝜓 . But a more subtle problem is that this estimator assumes that absence of evidence is evidence of absence. That is, it can estimate  𝜓̂   equals zero or one, and will tend to do so when  𝑟+𝑞  is small.

# In[289]:


tissue=args.tissue
tissue_file=args.tissue_file
filter_file=args.filter_file





counts=pd.read_csv(tissue_file, compression='gzip', sep=' ', index_col=0)

alt_splice_sites=pd.read_csv(filter_file, compression='gzip')

splicing_events_info=alt_splice_sites[alt_splice_sites.tissue==tissue]

counts_filtered_from_protein_coding=counts[counts.index.isin(splicing_events_info.ID)]



def find_PSI_values(a, b, a_prior, b_prior, FPR):

    nA=a
    nB=b

    a_post=a_prior+nA
    b_post=b_prior+nB

    posteriors=scipy.stats.beta(a_post,b_post)

    credible_intervals_low = posteriors.ppf(FPR)
    credible_intervals_high = posteriors.ppf(1-FPR)
 
    #for PSI calculations

    PSI_estimates = posteriors.mean()
    
    return PSI_estimates, credible_intervals_low, credible_intervals_high


# run across all exons in a given tissue
# --

# In[290]:


def estimate_uncertainty_around_EF(numerator, denominator, FPR):
    
    [EF_estimates, EF_credible_intervals_low, Ef_credible_intervals_high]=find_PSI_values(numerator,denominator, 1, 1,FPR)


    size_of_interval = Ef_credible_intervals_high - EF_credible_intervals_low

    
    
    return size_of_interval


def filter_for_two_intron_clusters(introns_in_given_tissue):
    
    value_counts_db= introns_in_given_tissue.groupby(['ID_Just_Number']).ID_Just_Number.value_counts().to_frame(name='num')


    has_two_introns_in_cluster=value_counts_db[value_counts_db['num']==2]
    
    list_of_tissues_and_IDs_w_two_introns=pd.Series(list(has_two_introns_in_cluster.index))
    

    return introns_in_given_tissue[introns_in_given_tissue.ID_Just_Number.isin(list_of_tissues_and_IDs_w_two_introns)]

counts_given_tissue_interpretable=counts



###############1########

introns_in_given_tissue=alt_splice_sites[alt_splice_sites.tissue==tissue]


introns_in_given_tissue=filter_for_two_intron_clusters(introns_in_given_tissue)


#find a event and separate based on length
longest_intron_in_cluster=introns_in_given_tissue.groupby('ID_Just_Number').i_length.max().to_frame().reset_index()

longest_introns_in_given_tissue=introns_in_given_tissue.merge(longest_intron_in_cluster,on=['ID_Just_Number','i_length'],how='inner')
#print(longest_introns_in_given_tissue)
shortest_intron_in_cluster=introns_in_given_tissue.groupby('ID_Just_Number').i_length.min().to_frame().reset_index()

shortest_introns_in_given_tissue=introns_in_given_tissue.merge(shortest_intron_in_cluster,on=['ID_Just_Number','i_length'], how='inner')

#print(shortest_introns_in_given_tissue)
#print(counts_given_tissue_interpretable)
#DIFFERENCE HERE: SWITCH A AND B TO OPP OF SHORT INTRON SCRIPT
long=counts_given_tissue_interpretable[counts_given_tissue_interpretable.index.isin(longest_introns_in_given_tissue.ID)]
#print(long)
short=counts_given_tissue_interpretable[counts_given_tissue_interpretable.index.isin(shortest_introns_in_given_tissue.ID)]

total_reads_supporting_short=short.sum(axis=1).reset_index(drop=True)

total_reads_supporting_long=long.sum(axis=1).reset_index(drop=True)

#pick the exon with less total reads across all individuals 
shorter_is_lower_PSI=total_reads_supporting_short<total_reads_supporting_long


#assign a and b introns based on which will be lower
a=pd.concat([short[shorter_is_lower_PSI.values], long[~shorter_is_lower_PSI.values]])

b=pd.concat([short[~shorter_is_lower_PSI.values], long[shorter_is_lower_PSI.values]])

a_raw=a.sort_index().values
b_raw=b.sort_index().values

a_w_names=a.sort_index()
b_w_names=b.sort_index()


def compute_log_likelihood(k, n, params):
    # compute log( \prod_j \sum_i pi_i pmf(k_j, n_j, a, b) )
    n_observations = k.shape[0]
    n_components = params.shape[0] // 3

    LogL = 0
    for i in range(n_components):
        a, b, pi = params[i], params[i + n_components], params[i + 2*n_components]
        #LogL += np.log(pi) + logsumexp( scipy.stats.betabinom.logpmf(k, n=n, a=a, b=b) )
        LogL += np.log(pi) +  scipy.stats.betabinom.logpmf(k, n=n, a=a, b=b).sum() 
    return LogL

def method_lbfgs(k, n, weights=None):
    obj_func_ = lambda x, k, n: -np.average(
                                  scipy.stats.betabinom.logpmf(
                                        #exon a reads
                                        k,
                                        #a+b reads
                                        n=n,
                                        #alpha
                                        a=2.**x[0],
                                        # beta
                                        b=2.**x[1]
                                  ), weights=weights)
    bounds = None

    obj_func = lambda x : np.arcsinh(obj_func_(x, k, n))

    x0=np.ones(2)
    minimized_a_b = minimize(obj_func, x0=x0, method='L-BFGS-B', bounds=bounds, options={'maxiter':100},
                            tol=1e-6) #change back to 100 when done debugging
    
    [a_est,b_est] = 2**minimized_a_b.x
    params = np.array([a_est, b_est, 1])
    return params, minimized_a_b

def method_lbfgs_em(k, n, n_components, em_iterations = 100):
    # params = [a1, a2, a3, b1, b2, b3, pi1, pi2, pi3]
    # a, b, pi = params[k], params[k + self.n_components], params[k + 2 * self.n_components]
    # initialize
    n_observations = k.shape[0]

    params = np.zeros(3*n_components)
    for i in range(n_components):
        params[i] = 1 + i
        params[i + n_components] = 1 + n_components - i
    params[2*n_components:] = 1/n_components
    gamma = np.zeros((n_components, n_observations))
    LogL = []
    minimized_a_b_function=[]
    if n_components == 1:
        em_iterations = 1
    for iter in range(em_iterations):
        # E step
        for i in range(n_components):
            a, b, pi = params[i], params[i + n_components], params[i + 2*n_components]
            gamma[i, :] = pi * scipy.stats.betabinom.pmf(k, n=n, a=a, b=b)
        gamma /= gamma.sum(axis=0)
        
        # M step
        params[2*n_components:3*n_components] = gamma.mean(axis=1)
        for i in range(n_components):
            weights = gamma[i, :]
            (params[i], params[i + n_components], _), obj_func = method_lbfgs(k, n, weights)
        minimized_a_b_function.append(obj_func)
        LogL.append(compute_log_likelihood(k, n, params))
    return params, LogL, minimized_a_b_function

# Function
# --

# In[324]:
def est_mixture_of_alphas_and_betas_w_restarts(exon_a, exon_b, n_components, em_iterations = 100):

        #drop any totally zero values in calculation
    a=exon_a[(exon_b+exon_a)>0]
    b=exon_b[(exon_b+exon_a)>0]

    psi = (a + 1) / (a + b + 2)
    n = a+b
    k = a

    for attempts in range(3):
        try:
            params, LogL, minimized_a_b_function = method_lbfgs_em(k, n, n_components, em_iterations)
            break
        except RuntimeWarning:
            continue
    else:
        params, LogL = None, None, None


       # break
    return params, LogL, minimized_a_b_function

def est_alphas_and_betas(counts_df_a, counts_df_b, power_transform, arcsin_transform, num_jobs, job_index):
    i=0

    FPR=0.05


    cluster_names=[]
    output_of_min_funcs=[]
    sum_of_all_reads=[]
    est_alphas_and_betas_list=[]
    est_LL=[]

    a_raw=counts_df_a.values
    b_raw=counts_df_b.values
    
    dip_results_pval=[]
    
    intron_1=[]
    intron_2=[]
    
    for exon_a,exon_b in tqdm(zip(a_raw, b_raw)):
        cluster_name=counts_df_a.index[i]+'_'+counts_df_b.index[i]
        intron_1_single=counts_df_a.index[i]
        intron_2_single=counts_df_b.index[i]

        
        i+=1
        if i % num_jobs != job_index:
            continue
    
        
       
        #drop any totally zero values in calculation
        a=exon_a[(exon_b+exon_a)>0]

        b=exon_b[(exon_b+exon_a)>0]
        # added puesdo count
        psi = (a + 1) / (a + b + 2)
        n=a+b
        sum_of_all_reads.append(n.sum())

        
        # perform the diptest across all PSI when there are are at least 1 count across both exons
        stat,pval=diptest.diptest(psi)
        dip_results_pval.append(pval)
        
        

############################# 1. calculate empirical bayes using chosen method depending on diptest ############################

        #intialize priors
        x0=np.ones(2)
        
        #minimize function of single component if and only if the diptest is not sigificant (BF corrected in tissue)
# minimize function of single component if and only if the diptest is not significant (BF corrected in tissue)
        if pval >= 1/len(a_raw):
            params, Log_Likelihood, minimized_a_b = est_mixture_of_alphas_and_betas_w_restarts(exon_a, exon_b, 1)
        
        # perform the EM algorithm with 2 components if the diptest is significant (assumption of unimodality can be rejected)
        else:
            double_params, Log_Likelihood, minimized_a_b = est_mixture_of_alphas_and_betas_w_restarts(exon_a, exon_b, 2)
            params=double_params
            # if one of alphas and betas are under 1, then there is a u-shaped curve. then try a triple:

            #params_order=[alpha1,alpha2,beta1,beta2,w1,w2]
            if (double_params[0] < 1 & double_params[2] < 1) | (double_params[1] < 1 & double_params[2] < 1):
                triple_params, Log_Likelihood, minimized_a_b = est_mixture_of_alphas_and_betas_w_restarts(exon_a, exon_b, 3)
                params=triple_params

        output_of_min_funcs.append(minimized_a_b)

        est_alphas_and_betas_list.append(params)
        
        est_LL.append(Log_Likelihood)

        intron_1.append(intron_1_single)

        intron_2.append(intron_2_single)

        cluster_names.append(cluster_name)

  
    
    not_sucesses=np.where([fun.success==False for fun in output_of_min_funcs[-1]])


    print('there are '+str(len(not_sucesses))+' minimizations that dont converge!')
    print(not_sucesses)


    
    return est_alphas_and_betas_list, output_of_min_funcs, cluster_names, est_LL, sum_of_all_reads, intron_1, intron_2
        
def calc_exon_frequencies(counts_df_a, counts_df_b, a_estimates, b_estimates, lower_bound, FPR):
        exon_frequencies=[]
  
        passed_filtering=[]
        EF_lower_bound=[]
        EF_upper_bound=[]
        a_raw=counts_df_a.values
        b_raw=counts_df_b.values
        i=0

        for exon_a,exon_b in tqdm(zip(a_raw, b_raw)):
            
            a_est=a_estimates[i]
            b_est=b_estimates[i]
           



# # run emp bayes to generate alphas and betas
num_jobs=args.num_jobs
job_index=args.job_index

[est_alphas_and_betas_list, output_of_min_funcs, cluster_names, est_LL, sum_of_all_reads, intron_1, intron_2 ] = est_alphas_and_betas(a_w_names,b_w_names, True,True, num_jobs, job_index)









#set up output df
#itemgetter(*passed_filtering_5)
output_df=pd.DataFrame({ 'intron_1': intron_1,
                'intron_2': intron_2,  
                'total_reads_spanning_all_junctions': sum_of_all_reads, 
                'function_output_emp_bayes':output_of_min_funcs,
                       'cluster_name':cluster_names,
                       'number_people_in_sample':len(a_w_names.columns),
                'params': est_alphas_and_betas_list,
                        'LogLikelihood':est_LL})





#write output df
with open(args.output_file, 'w') as output_file:
    output_df.to_csv(output_file)
    
